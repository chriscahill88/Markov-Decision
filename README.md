# Markov-Decision
Code: https://github.com/chriscahill88/Markov-Decision

Image policies and value grids: The Horizon values and which Grid was used is below each photo
     Value Iteration Horizon 100 Grid 1 
Value Iteration Horizon 50 Grid 1 
Value Iteration  Horizon 50 Grid 2 
 Value Iteration Horizon 100 Grid 2 
Epsilon Greedy Q learning Grid 1
 
Epsilon Greedy Q learning Grid 1
 
Epsilon Greedy Q learning Grid 1
 
 
Epsilon Greedy Q learning Grid 2

 
Epsilon Greedy Q learning Grid 2 
Epsilon Greedy Q learning Grid 2

Epsilon Greedy Q learning Grid 2 

Epsilon Greedy Q learning Grid 2 
Epsilon Greedy Q learning Grid 1
 
Epsilon Greedy Q learning Grid 1
 
Policy Iteration Horizon 100 Grid 2 
Policy Iteration Horizon 50 Grid 2 
Policy Iteration Horizon 50 Grid 1 
Policy Iteration Horizon 100 Grid 1

What I learned: I learned a lot from this project. First off I learned how Markov decisions can be solved and learned what Value and Policy Iteration were and also what Epsilon Greedy Q Learning and how all of them worked. More specifically I learned during Value and Policy iteration how Bellmanâ€™s equation works and how those values are assigned and are then used to find the optimal route.  I also learned about Policy Iteration and how it uses policy improvement to find the optimal route. I learned how Epsilon Greedy Q Learning uses Exploration vs Exploitation at each step and then uses this to select an action on the grid. I find that the Exploration vs Exploitation trade-off is very basic but allows for different routes to be found every time. I learned how the hyperparameters impact the equations and the routes and values found. The hyperparameters can either reduce the amount of iterations but make the route less optimal or vice-versa. 
![image](https://github.com/chriscahill88/Markov-Decision/assets/43705092/f6d3bc8d-de6f-4b5d-8dd7-3048d970f7de)
